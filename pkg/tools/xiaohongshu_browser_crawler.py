"""
å°çº¢ä¹¦æµè§ˆå™¨è‡ªåŠ¨åŒ–çˆ¬è™«å·¥å…·

é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–å®æ—¶æ“ä½œæµè§ˆå™¨è·å–å°çº¢ä¹¦å†…å®¹ã€‚
æ”¯æŒï¼š
1. ä½¿ç”¨å·²ç™»å½•çš„æµè§ˆå™¨ä¼šè¯
2. æœç´¢å…³é”®è¯
3. è·å–å¸–å­å®Œæ•´å†…å®¹ï¼ˆåŒ…æ‹¬é£Ÿæã€åšæ³•ç­‰ï¼‰
4. åˆ†ç±»ä¿å­˜åˆ°æœ¬åœ°

ä½¿ç”¨ Playwright è¿›è¡Œæµè§ˆå™¨è‡ªåŠ¨åŒ–æ“ä½œã€‚
"""

import asyncio
import json
import os
import re
import hashlib
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict, field


@dataclass
class CrawledRecipe:
    """çˆ¬å–çš„èœè°±æ•°æ®"""

    # åŸºç¡€ä¿¡æ¯
    title: str
    author: str
    date: str
    likes: str
    url: str

    # åˆ†ç±»ä¿¡æ¯
    category: str = ""
    age_group: str = ""

    # è¯¦ç»†å†…å®¹
    description: str = ""
    ingredients: List[str] = field(default_factory=list)
    steps: List[str] = field(default_factory=list)
    tips: List[str] = field(default_factory=list)

    # åª’ä½“å†…å®¹
    images: List[str] = field(default_factory=list)
    local_images: List[str] = field(default_factory=list)

    # é¢å¤–ä¿¡æ¯
    tags: List[str] = field(default_factory=list)
    comments_count: str = "0"
    collect_count: str = "0"

    # å…ƒæ•°æ®
    crawled_at: str = ""
    content_hash: str = ""


class XiaohongshuBrowserCrawler:
    """å°çº¢ä¹¦æµè§ˆå™¨è‡ªåŠ¨åŒ–çˆ¬è™«"""

    # å¹´é¾„åˆ†ç±»
    AGE_GROUPS = {
        "6æœˆé¾„": ["6æœˆ", "å…­æœˆ", "6ä¸ªæœˆ"],
        "7-8æœˆé¾„": ["7æœˆ", "8æœˆ", "ä¸ƒæœˆ", "å…«æœˆ", "7ä¸ªæœˆ", "8ä¸ªæœˆ"],
        "9-10æœˆé¾„": ["9æœˆ", "10æœˆ", "ä¹æœˆ", "åæœˆ", "9ä¸ªæœˆ", "10ä¸ªæœˆ"],
        "11-12æœˆé¾„": ["11æœˆ", "12æœˆ", "åä¸€æœˆ", "åäºŒæœˆ", "11ä¸ªæœˆ", "12ä¸ªæœˆ"],
        "1å²ä»¥ä¸Š": ["1å²", "ä¸€å²", "ä¸€å‘¨å²", "12æœˆé¾„+", "1å²+"],
    }

    # é£Ÿç‰©ç±»å‹åˆ†ç±»
    FOOD_TYPES = {
        "ä¸»é£Ÿç±»": ["ç±³ç³Š", "ç±³ç²‰", "ç²¥", "é¢æ¡", "é¥­", "çƒ©é¥­", "é¢", "ç±³"],
        "è›‹ç™½è´¨ç±»": ["è‚‰", "ç‰›è‚‰", "é¸¡è‚‰", "çŒªè‚‰", "é±¼", "è™¾", "è›‹", "è±†è…"],
        "è”¬èœç±»": ["è”¬èœ", "èœ", "èƒ¡èåœ", "å—ç“œ", "åœŸè±†", "å±±è¯", "è¥¿å…°èŠ±", "è èœ"],
        "æ°´æœç±»": ["æ°´æœ", "è‹¹æœ", "é¦™è•‰", "æ¢¨", "æ©™", "è‰è“"],
        "æ‰‹æŒ‡é£Ÿç‰©": ["æ‰‹æŒ‡", "æ¡", "æ£’", "å—", "å°é¥¼"],
        "æ±¤ç¾¹ç±»": ["æ±¤", "ç¾¹", "ç³Š"],
        "çƒ˜ç„™ç±»": ["æ¾é¥¼", "è›‹ç³•", "é¥¼å¹²", "è’¸ç³•", "ç³•"],
    }

    def __init__(
        self,
        output_dir: str = "baby_food_recipes",
        headless: bool = False,
        slow_mo: int = 100,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            output_dir: è¾“å‡ºç›®å½•
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨
            slow_mo: æ“ä½œå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºè°ƒè¯•
        """
        self.output_dir = output_dir
        self.headless = headless
        self.slow_mo = slow_mo
        self.recipes: List[CrawledRecipe] = []
        self.browser = None
        self.context = None
        self.page = None
        self._ensure_output_dir()

    def _ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "recipes"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "images"), exist_ok=True)

    def _classify_age_group(self, text: str) -> str:
        """æ ¹æ®æ–‡æœ¬åˆ†ç±»å¹´é¾„ç»„"""
        for age_group, keywords in self.AGE_GROUPS.items():
            for keyword in keywords:
                if keyword in text:
                    return age_group
        return "é€šç”¨"

    def _classify_food_type(self, text: str) -> str:
        """æ ¹æ®æ–‡æœ¬åˆ†ç±»é£Ÿç‰©ç±»å‹"""
        for food_type, keywords in self.FOOD_TYPES.items():
            for keyword in keywords:
                if keyword in text:
                    return food_type
        return "å…¶ä»–"

    def _generate_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()[:12]

    def _parse_ingredients(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­è§£æé£Ÿæåˆ—è¡¨"""
        ingredients = []
        patterns = [
            r"é£Ÿæ[ï¼š:]\s*(.+?)(?=åšæ³•|æ­¥éª¤|$)",
            r"ææ–™[ï¼š:]\s*(.+?)(?=åšæ³•|æ­¥éª¤|$)",
            r"ç”¨æ–™[ï¼š:]\s*(.+?)(?=åšæ³•|æ­¥éª¤|$)",
            r"å‡†å¤‡[ï¼š:]\s*(.+?)(?=åšæ³•|æ­¥éª¤|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                ingredient_text = match.group(1)
                items = re.split(r"[,ï¼Œã€\n\r]+", ingredient_text)
                for item in items:
                    item = item.strip()
                    if item and len(item) < 50:
                        ingredients.append(item)
                break

        return ingredients

    def _parse_steps(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­è§£æåˆ¶ä½œæ­¥éª¤"""
        steps = []
        patterns = [
            r"åšæ³•[ï¼š:]\s*(.+?)(?=å°è´´å£«|tips|$)",
            r"æ­¥éª¤[ï¼š:]\s*(.+?)(?=å°è´´å£«|tips|$)",
            r"åˆ¶ä½œ[ï¼š:]\s*(.+?)(?=å°è´´å£«|tips|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                steps_text = match.group(1)
                step_items = re.split(r"(?:\d+[.ã€)ï¼‰]|\n)", steps_text)
                for item in step_items:
                    item = item.strip()
                    if item and len(item) > 5:
                        steps.append(item)
                break

        if not steps:
            lines = text.split("\n")
            for line in lines:
                line = line.strip()
                if re.match(r"^\d+[.ã€)ï¼‰]", line) or any(
                    verb in line for verb in ["åŠ å…¥", "æ…æ‹Œ", "è’¸", "ç…®", "åˆ‡", "æ”¾å…¥"]
                ):
                    if len(line) > 5:
                        steps.append(re.sub(r"^\d+[.ã€)ï¼‰]\s*", "", line))

        return steps

    def _parse_tips(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­è§£æå°è´´å£«"""
        tips = []
        patterns = [
            r"(?:å°è´´å£«|tips|æ¸©é¦¨æç¤º|æ³¨æ„)[ï¼š:]\s*(.+?)$",
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                tips_text = match.group(1)
                items = re.split(r"[ã€‚\n]+", tips_text)
                for item in items:
                    item = item.strip()
                    if item and len(item) > 5:
                        tips.append(item)
                break

        return tips

    def _parse_tags(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­è§£ææ ‡ç­¾"""
        tags = re.findall(r"#([^#\s]+)#?", text)
        return list(set(tags))

    async def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        try:
            from playwright.async_api import async_playwright

            self.playwright = await async_playwright().start()

            # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œä¿ç•™ç™»å½•çŠ¶æ€
            user_data_dir = os.path.expanduser("~/.xiaohongshu_browser_data")
            os.makedirs(user_data_dir, exist_ok=True)

            print("[Browser] æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")

            self.context = await self.playwright.chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                headless=self.headless,
                slow_mo=self.slow_mo,
                viewport={"width": 1280, "height": 800},
                locale="zh-CN",
                args=[
                    "--disable-blink-features=AutomationControlled",
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                ],
            )

            # è®¾ç½®é»˜è®¤è¶…æ—¶
            self.context.set_default_timeout(60000)
            self.context.set_default_navigation_timeout(60000)

            # åˆ›å»ºæ–°é¡µé¢
            self.page = await self.context.new_page()

            # è®¾ç½®é¡µé¢è¶…æ—¶
            self.page.set_default_timeout(60000)
            self.page.set_default_navigation_timeout(60000)

            print("[Browser] âœ… æµè§ˆå™¨å·²å¯åŠ¨")
            return True

        except ImportError:
            print("[Browser] âŒ é”™è¯¯: è¯·å…ˆå®‰è£… playwright")
            print("è¿è¡Œ: pip install playwright && playwright install chromium")
            return False
        except Exception as e:
            print(f"[Browser] âŒ å¯åŠ¨æµè§ˆå™¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def _close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.context:
            await self.context.close()
        if hasattr(self, "playwright") and self.playwright:
            await self.playwright.stop()
        print("[Browser] æµè§ˆå™¨å·²å…³é—­")

    async def check_login(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        try:
            print("[Browser] æ­£åœ¨è®¿é—®å°çº¢ä¹¦...")
            await self.page.goto(
                "https://www.xiaohongshu.com/explore",
                wait_until="domcontentloaded",
                timeout=30000,
            )
            print("[Browser] é¡µé¢å·²åŠ è½½ï¼Œç­‰å¾…å†…å®¹æ¸²æŸ“...")
            await asyncio.sleep(3)  # ç­‰å¾…é¡µé¢å®Œå…¨æ¸²æŸ“

            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å¤´åƒæˆ–"æˆ‘"æŒ‰é’®ï¼ˆå·²ç™»å½•æ ‡å¿—ï¼‰
            user_link = await self.page.query_selector('a[href*="/user/profile/"]')
            if user_link:
                print("[Browser] âœ… å·²ç™»å½•")
                return True

            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•æ ‡å¿—ï¼‰
            login_btn = await self.page.query_selector('button:has-text("ç™»å½•")')
            if login_btn:
                print("[Browser] âŒ æœªç™»å½•")
                return False

            # æ£€æŸ¥æœç´¢æ¡†æç¤ºæ–‡å­—
            search_box = await self.page.query_selector('input[placeholder*="ç™»å½•"]')
            if search_box:
                print("[Browser] âŒ æœªç™»å½•ï¼ˆéœ€è¦ç™»å½•æ¢ç´¢æ›´å¤šå†…å®¹ï¼‰")
                return False

            print("[Browser] âš ï¸ æ— æ³•ç¡®å®šç™»å½•çŠ¶æ€ï¼Œå°è¯•ç»§ç»­...")
            return True

        except Exception as e:
            print(f"[Browser] æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def trigger_login(self):
        """è§¦å‘ç™»å½•å¼¹çª—"""
        try:
            # æ–¹æ³•1: ç‚¹å‡»ç™»å½•æŒ‰é’®
            login_btn = await self.page.query_selector('button:has-text("ç™»å½•")')
            if login_btn:
                await login_btn.click()
                await asyncio.sleep(1)
                print("[Browser] ğŸ“± å·²æ‰“å¼€ç™»å½•å¼¹çª—")
                return True

            # æ–¹æ³•2: ç‚¹å‡»æœç´¢æ¡†è§¦å‘ç™»å½•
            search_box = await self.page.query_selector('input[placeholder*="ç™»å½•"]')
            if search_box:
                await search_box.click()
                await asyncio.sleep(1)
                print("[Browser] ğŸ“± å·²è§¦å‘ç™»å½•å¼¹çª—")
                return True

            # æ–¹æ³•3: å°è¯•è®¿é—®éœ€è¦ç™»å½•çš„é¡µé¢
            await self.page.goto("https://www.xiaohongshu.com/notification")
            await asyncio.sleep(2)
            print("[Browser] ğŸ“± å·²è·³è½¬åˆ°éœ€è¦ç™»å½•çš„é¡µé¢")
            return True

        except Exception as e:
            print(f"[Browser] è§¦å‘ç™»å½•å¤±è´¥: {e}")
            return False

    async def wait_for_login(self, timeout: int = 300):
        """ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•

        ä½¿ç”¨ Playwright çš„ pause() åŠŸèƒ½ï¼Œè®©ç”¨æˆ·å¯ä»¥åœ¨æµè§ˆå™¨ä¸­è‡ªç”±æ“ä½œã€‚

        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        print()
        print("=" * 60)
        print("ğŸ“± éœ€è¦ç™»å½•å°çº¢ä¹¦")
        print("=" * 60)
        print()
        print("ğŸ”” é‡è¦æç¤º:")
        print("   1. æµè§ˆå™¨å·²æ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨çª—å£ä¸­æ“ä½œ")
        print("   2. ç‚¹å‡»ã€Œç™»å½•ã€æŒ‰é’®")
        print("   3. ä½¿ç”¨å°çº¢ä¹¦Appæ‰«æäºŒç»´ç ")
        print("   4. ç™»å½•æˆåŠŸåï¼Œåœ¨ç»ˆç«¯æŒ‰ Enter é”®ç»§ç»­")
        print()
        print("=" * 60)

        # è§¦å‘ç™»å½•å¼¹çª—
        await self.trigger_login()

        # æš‚åœç¨‹åºï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ“ä½œ
        # ä½¿ç”¨ input() è€Œä¸æ˜¯ page.pause()ï¼Œè¿™æ ·æ›´ç®€å•
        print()
        input("ğŸ‘† è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åæŒ‰ Enter é”®ç»§ç»­...")
        print()

        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        await asyncio.sleep(2)
        user_link = await self.page.query_selector('a[href*="/user/profile/"]')
        if user_link:
            print("[Browser] âœ… ç™»å½•æˆåŠŸï¼")
            return True

        # å¦‚æœè¿˜æ²¡ç™»å½•ï¼Œå†ç»™ç”¨æˆ·ä¸€æ¬¡æœºä¼š
        print("[Browser] âš ï¸ ä¼¼ä¹è¿˜æœªç™»å½•ï¼Œå†æ£€æŸ¥ä¸€æ¬¡...")
        await self.page.reload()
        await asyncio.sleep(3)

        user_link = await self.page.query_selector('a[href*="/user/profile/"]')
        if user_link:
            print("[Browser] âœ… ç™»å½•æˆåŠŸï¼")
            return True

        print("[Browser] âŒ ç™»å½•å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®æ‰«ç ç™»å½•")
        retry = input("æ˜¯å¦é‡è¯•ï¼Ÿ(y/n): ").strip().lower()
        if retry == 'y':
            return await self.wait_for_login(timeout)

        return False

    async def search(self, keyword: str) -> List[Dict[str, str]]:
        """æœç´¢å…³é”®è¯

        Args:
            keyword: æœç´¢å…³é”®è¯

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        print(f"[Browser] æœç´¢: {keyword}")

        # å¯¼èˆªåˆ°æœç´¢é¡µé¢
        search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}"
        await self.page.goto(search_url)
        await self.page.wait_for_load_state("networkidle")
        await asyncio.sleep(2)  # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½

        # è·å–æœç´¢ç»“æœ
        results = []
        note_cards = await self.page.query_selector_all('section[class*="note-item"]')

        if not note_cards:
            # å°è¯•å…¶ä»–é€‰æ‹©å™¨
            note_cards = await self.page.query_selector_all('div[class*="note-item"]')

        if not note_cards:
            # å°è¯•é€šç”¨é“¾æ¥é€‰æ‹©å™¨
            note_cards = await self.page.query_selector_all(
                'a[href*="/explore/"], a[href*="/search_result/"]'
            )

        print(f"[Browser] æ‰¾åˆ° {len(note_cards)} ä¸ªç»“æœ")

        for card in note_cards[:20]:  # é™åˆ¶æ•°é‡
            try:
                # è·å–é“¾æ¥
                href = await card.get_attribute("href")
                if not href:
                    link = await card.query_selector("a")
                    if link:
                        href = await link.get_attribute("href")

                if not href:
                    continue

                # æ„å»ºå®Œæ•´URL
                if href.startswith("/"):
                    href = f"https://www.xiaohongshu.com{href}"

                # è·å–æ ‡é¢˜
                title_elem = await card.query_selector(
                    'span[class*="title"], div[class*="title"], .note-content'
                )
                title = ""
                if title_elem:
                    title = await title_elem.inner_text()

                # è·å–ä½œè€…
                author_elem = await card.query_selector(
                    'span[class*="author"], a[class*="author"], .author-name'
                )
                author = ""
                if author_elem:
                    author = await author_elem.inner_text()

                # è·å–ç‚¹èµæ•°
                likes_elem = await card.query_selector(
                    'span[class*="like"], span[class*="count"]'
                )
                likes = "0"
                if likes_elem:
                    likes = await likes_elem.inner_text()

                if title or href:
                    results.append(
                        {
                            "title": title.strip() if title else "",
                            "author": author.strip() if author else "",
                            "url": href,
                            "likes": likes.strip() if likes else "0",
                        }
                    )

            except Exception as e:
                print(f"[Browser] è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
                continue

        return results

    async def get_note_detail(self, url: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¬”è®°è¯¦æƒ…

        Args:
            url: ç¬”è®°URL

        Returns:
            ç¬”è®°è¯¦æƒ…
        """
        try:
            print(f"[Browser] è·å–ç¬”è®°: {url[:50]}...")

            # æ‰“å¼€æ–°æ ‡ç­¾é¡µ
            page = await self.context.new_page()
            await page.goto(url)
            await page.wait_for_load_state("networkidle")
            await asyncio.sleep(2)  # ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½

            detail = {"url": url}

            # è·å–æ ‡é¢˜
            title_elem = await page.query_selector(
                'div[class*="title"], h1, .note-content .title'
            )
            if title_elem:
                detail["title"] = await title_elem.inner_text()

            # è·å–ä½œè€…
            author_elem = await page.query_selector(
                'a[class*="author"], .author-name, a[href*="/user/profile/"] span'
            )
            if author_elem:
                detail["author"] = await author_elem.inner_text()

            # è·å–æ­£æ–‡å†…å®¹
            content_elem = await page.query_selector(
                'div[class*="desc"], div[class*="content"], .note-text'
            )
            if content_elem:
                detail["description"] = await content_elem.inner_text()

            # è·å–å‘å¸ƒæ—¥æœŸ
            date_elem = await page.query_selector(
                'span[class*="date"], span[class*="time"], .publish-time'
            )
            if date_elem:
                detail["date"] = await date_elem.inner_text()

            # è·å–ç‚¹èµæ•°
            likes_elem = await page.query_selector(
                'span[class*="like-count"], span[class*="count"]:first-of-type'
            )
            if likes_elem:
                detail["likes"] = await likes_elem.inner_text()

            # è·å–è¯„è®ºæ•°
            comments_elem = await page.query_selector(
                'span[class*="comment-count"], span[class*="count"]:nth-of-type(2)'
            )
            if comments_elem:
                detail["comments_count"] = await comments_elem.inner_text()

            # è·å–æ”¶è—æ•°
            collect_elem = await page.query_selector(
                'span[class*="collect-count"], span[class*="count"]:nth-of-type(3)'
            )
            if collect_elem:
                detail["collect_count"] = await collect_elem.inner_text()

            # è·å–å›¾ç‰‡
            img_elems = await page.query_selector_all(
                'img[class*="note-image"], img[class*="swiper"], .carousel img'
            )
            images = []
            for img in img_elems:
                src = await img.get_attribute("src")
                if src and "xhscdn" in src:
                    images.append(src)
            detail["images"] = images

            # è·å–æ ‡ç­¾
            tag_elems = await page.query_selector_all('a[href*="keyword="]')
            tags = []
            for tag in tag_elems:
                tag_text = await tag.inner_text()
                if tag_text.startswith("#"):
                    tags.append(tag_text.lstrip("#"))
            detail["tags"] = tags

            await page.close()
            return detail

        except Exception as e:
            print(f"[Browser] è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥: {e}")
            return None

    def _create_recipe_from_detail(self, detail: Dict[str, Any]) -> CrawledRecipe:
        """ä»è¯¦æƒ…åˆ›å»ºèœè°±å¯¹è±¡"""
        description = detail.get("description", "")
        title = detail.get("title", "")
        full_text = f"{title} {description}"

        return CrawledRecipe(
            title=title,
            author=detail.get("author", ""),
            date=detail.get("date", ""),
            likes=detail.get("likes", "0"),
            url=detail.get("url", ""),
            category=self._classify_food_type(full_text),
            age_group=self._classify_age_group(full_text),
            description=description,
            ingredients=self._parse_ingredients(description),
            steps=self._parse_steps(description),
            tips=self._parse_tips(description),
            images=detail.get("images", []),
            tags=detail.get("tags", []) + self._parse_tags(description),
            comments_count=detail.get("comments_count", "0"),
            collect_count=detail.get("collect_count", "0"),
            crawled_at=datetime.now().isoformat(),
            content_hash=self._generate_content_hash(detail.get("url", "")),
        )

    async def crawl(
        self,
        keyword: str = "å®å®è¾…é£Ÿ",
        max_notes: int = 10,
        get_details: bool = True,
        login_timeout: int = 300,
    ) -> List[CrawledRecipe]:
        """æ‰§è¡Œçˆ¬å–

        Args:
            keyword: æœç´¢å…³é”®è¯
            max_notes: æœ€å¤§çˆ¬å–æ•°é‡
            get_details: æ˜¯å¦è·å–è¯¦æƒ…
            login_timeout: ç™»å½•ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            çˆ¬å–çš„èœè°±åˆ—è¡¨
        """
        # åˆå§‹åŒ–æµè§ˆå™¨
        if not await self._init_browser():
            return []

        try:
            print()
            print("[Browser] ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")

            # æ£€æŸ¥ç™»å½•çŠ¶æ€
            is_logged_in = await self.check_login()

            if not is_logged_in:
                print("[Browser] ğŸ“± éœ€è¦ç™»å½•ï¼Œæ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢...")

                # ç­‰å¾…ç”¨æˆ·ç™»å½•
                if not await self.wait_for_login(timeout=login_timeout):
                    print("[Browser] âŒ ç™»å½•å¤±è´¥ï¼Œç»ˆæ­¢çˆ¬å–")
                    return []

                # ç™»å½•æˆåŠŸåï¼Œé‡æ–°å¯¼èˆªåˆ°é¦–é¡µ
                print("[Browser] ğŸ”„ ç™»å½•æˆåŠŸï¼Œæ­£åœ¨å‡†å¤‡æœç´¢...")
                await self.page.goto("https://www.xiaohongshu.com/explore")
                await asyncio.sleep(2)

            print()
            print(f"[Browser] ğŸ” å¼€å§‹æœç´¢: {keyword}")

            # æœç´¢
            search_results = await self.search(keyword)
            print(f"[Browser] è·å–åˆ° {len(search_results)} ä¸ªæœç´¢ç»“æœ")

            # é™åˆ¶æ•°é‡
            search_results = search_results[:max_notes]

            # è·å–è¯¦æƒ…
            for i, result in enumerate(search_results):
                print(
                    f"[Browser] å¤„ç† {i + 1}/{len(search_results)}: {result.get('title', '')[:30]}..."
                )

                if get_details:
                    detail = await self.get_note_detail(result["url"])
                    if detail:
                        # åˆå¹¶æœç´¢ç»“æœå’Œè¯¦æƒ…
                        merged = {**result, **detail}
                        recipe = self._create_recipe_from_detail(merged)
                        self.recipes.append(recipe)
                else:
                    # åªä½¿ç”¨æœç´¢ç»“æœä¿¡æ¯
                    recipe = CrawledRecipe(
                        title=result.get("title", ""),
                        author=result.get("author", ""),
                        date="",
                        likes=result.get("likes", "0"),
                        url=result.get("url", ""),
                        crawled_at=datetime.now().isoformat(),
                        content_hash=self._generate_content_hash(result.get("url", "")),
                    )
                    self.recipes.append(recipe)

                # å»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬
                await asyncio.sleep(1)

            print(f"[Browser] çˆ¬å–å®Œæˆï¼Œå…± {len(self.recipes)} ä¸ªèœè°±")
            return self.recipes

        finally:
            await self._close_browser()

    def save_to_json(self, filename: str = "recipes.json") -> str:
        """ä¿å­˜ä¸ºJSON"""
        filepath = os.path.join(self.output_dir, filename)
        data = {
            "created_at": datetime.now().isoformat(),
            "source": "å°çº¢ä¹¦æµè§ˆå™¨è‡ªåŠ¨åŒ–çˆ¬å–",
            "total_count": len(self.recipes),
            "recipes": [asdict(r) for r in self.recipes],
        }
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"[Save] å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def save_to_markdown(self, filename: str = "èœè°±å¤§å…¨.md") -> str:
        """ä¿å­˜ä¸ºMarkdown"""
        filepath = os.path.join(self.output_dir, filename)

        lines = [
            "# ğŸ¼ å®å®è¾…é£Ÿèœè°±å¤§å…¨",
            "",
            "> æ•°æ®æ¥æºï¼šå°çº¢ä¹¦æµè§ˆå™¨è‡ªåŠ¨åŒ–çˆ¬å–",
            f"> æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"> å…±æ”¶å½• **{len(self.recipes)}** ä¸ªèœè°±",
            "",
            "---",
            "",
        ]

        for recipe in self.recipes:
            lines.append(f"## {recipe.title}")
            lines.append("")
            lines.append(f"- ğŸ‘¤ **ä½œè€…**ï¼š{recipe.author}")
            lines.append(f"- â¤ï¸ **ç‚¹èµ**ï¼š{recipe.likes}")
            lines.append(f"- ğŸ“ **åˆ†ç±»**ï¼š{recipe.category} | {recipe.age_group}")

            if recipe.tags:
                lines.append(
                    f"- ğŸ·ï¸ **æ ‡ç­¾**ï¼š{', '.join(['#' + t for t in recipe.tags])}"
                )

            lines.append(f"- ğŸ”— **åŸæ–‡**ï¼š[æŸ¥çœ‹åŸæ–‡]({recipe.url})")
            lines.append("")

            if recipe.description:
                lines.append("### ğŸ“ å†…å®¹")
                lines.append("")
                lines.append(recipe.description)
                lines.append("")

            if recipe.ingredients:
                lines.append("### ğŸ¥¬ é£Ÿæ")
                lines.append("")
                for ing in recipe.ingredients:
                    lines.append(f"- {ing}")
                lines.append("")

            if recipe.steps:
                lines.append("### ğŸ‘©â€ğŸ³ åˆ¶ä½œæ­¥éª¤")
                lines.append("")
                for i, step in enumerate(recipe.steps, 1):
                    lines.append(f"{i}. {step}")
                lines.append("")

            if recipe.tips:
                lines.append("### ğŸ’¡ å°è´´å£«")
                lines.append("")
                for tip in recipe.tips:
                    lines.append(f"- {tip}")
                lines.append("")

            lines.append("---")
            lines.append("")

        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print(f"[Save] å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def save_individual_recipes(self, subdir: str = "recipes") -> str:
        """ä¿å­˜å•ç‹¬çš„èœè°±æ–‡ä»¶"""
        recipes_dir = os.path.join(self.output_dir, subdir)
        os.makedirs(recipes_dir, exist_ok=True)

        for i, recipe in enumerate(self.recipes):
            safe_title = re.sub(r'[<>:"/\\|?*]', "", recipe.title)[:30]
            filename = f"{i + 1:02d}_{safe_title}.md"
            filepath = os.path.join(recipes_dir, filename)

            content = f"""# {recipe.title}

- ğŸ‘¤ ä½œè€…ï¼š{recipe.author}
- ğŸ“… æ—¥æœŸï¼š{recipe.date}
- â¤ï¸ ç‚¹èµï¼š{recipe.likes}
- ğŸ“ åˆ†ç±»ï¼š{recipe.category} | {recipe.age_group}
- ğŸ”— åŸæ–‡ï¼š[æŸ¥çœ‹åŸæ–‡]({recipe.url})

**æ ‡ç­¾**ï¼š{" ".join(["#" + t for t in recipe.tags])}

---

## ğŸ“ å†…å®¹

{recipe.description}
"""

            if recipe.ingredients:
                content += "\n## ğŸ¥¬ é£Ÿæ\n\n"
                for ing in recipe.ingredients:
                    content += f"- {ing}\n"

            if recipe.steps:
                content += "\n## ğŸ‘©â€ğŸ³ åˆ¶ä½œæ­¥éª¤\n\n"
                for j, step in enumerate(recipe.steps, 1):
                    content += f"{j}. {step}\n"

            if recipe.tips:
                content += "\n## ğŸ’¡ å°è´´å£«\n\n"
                for tip in recipe.tips:
                    content += f"- {tip}\n"

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(content)

        print(f"[Save] å·²ä¿å­˜ {len(self.recipes)} ä¸ªå•ç‹¬èœè°±åˆ°: {recipes_dir}")
        return recipes_dir

    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "=" * 50)
        print("ğŸ“Š å®å®è¾…é£Ÿèœè°±çˆ¬å–æ‘˜è¦")
        print("=" * 50)
        print(f"æ€»èœè°±æ•°: {len(self.recipes)}")

        with_content = sum(1 for r in self.recipes if r.description)
        with_ingredients = sum(1 for r in self.recipes if r.ingredients)
        with_steps = sum(1 for r in self.recipes if r.steps)

        print("\nğŸ“„ å†…å®¹ç»Ÿè®¡:")
        print(f"  - æœ‰è¯¦ç»†æè¿°: {with_content}ä¸ª")
        print(f"  - æœ‰é£Ÿæåˆ—è¡¨: {with_ingredients}ä¸ª")
        print(f"  - æœ‰åˆ¶ä½œæ­¥éª¤: {with_steps}ä¸ª")

        # æŒ‰å¹´é¾„åˆ†ç±»
        by_age: Dict[str, int] = {}
        for recipe in self.recipes:
            age = recipe.age_group or "é€šç”¨"
            by_age[age] = by_age.get(age, 0) + 1

        print("\nğŸ“… æŒ‰æœˆé¾„åˆ†ç±»:")
        for age, count in sorted(by_age.items()):
            print(f"  - {age}: {count}ä¸ª")

        # æŒ‰é£Ÿç‰©ç±»å‹åˆ†ç±»
        by_category: Dict[str, int] = {}
        for recipe in self.recipes:
            cat = recipe.category or "å…¶ä»–"
            by_category[cat] = by_category.get(cat, 0) + 1

        print("\nğŸ³ æŒ‰é£Ÿç‰©ç±»å‹åˆ†ç±»:")
        for cat, count in sorted(by_category.items()):
            print(f"  - {cat}: {count}ä¸ª")

        print("=" * 50 + "\n")


async def main():
    """ä¸»å‡½æ•°"""
    crawler = XiaohongshuBrowserCrawler(
        output_dir="baby_food_recipes",
        headless=False,  # è®¾ç½®ä¸ºFalseä»¥ä¾¿è§‚å¯Ÿå’Œæ‰‹åŠ¨ç™»å½•
        slow_mo=100,
    )

    # æ‰§è¡Œçˆ¬å–
    recipes = await crawler.crawl(
        keyword="å®å®è¾…é£Ÿ",
        max_notes=10,  # çˆ¬å–10ä¸ªå¸–å­
        get_details=True,
    )

    if recipes:
        # æ‰“å°æ‘˜è¦
        crawler.print_summary()

        # ä¿å­˜åˆ°æ–‡ä»¶
        crawler.save_to_json()
        crawler.save_to_markdown()
        crawler.save_individual_recipes()

        print("âœ… çˆ¬å–å®Œæˆï¼")
    else:
        print("âŒ æœªè·å–åˆ°ä»»ä½•å†…å®¹")


if __name__ == "__main__":
    asyncio.run(main())
